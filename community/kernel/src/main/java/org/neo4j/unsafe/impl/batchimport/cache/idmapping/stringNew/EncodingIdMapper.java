/*
 * Copyright (c) 2002-2017 "Neo Technology,"
 * Network Engine for Objects in Lund AB [http://neotechnology.com]
 *
 * This file is part of Neo4j.
 *
 * Neo4j is free software: you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation, either version 3 of the License, or
 * (at your option) any later version.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License
 * along with this program.  If not, see <http://www.gnu.org/licenses/>.
 */
package org.neo4j.unsafe.impl.batchimport.cache.idmapping.stringNew;

import java.io.File;
import java.io.FileInputStream;
import java.io.FileOutputStream;
import java.io.IOException;
import java.nio.ByteBuffer;
import java.nio.channels.FileChannel;
import java.util.Arrays;
import java.util.Collection;
import java.util.HashMap;
import java.util.function.LongFunction;

import org.neo4j.collection.primitive.PrimitiveLongCollections;
import org.neo4j.collection.primitive.PrimitiveLongIterator;
import org.neo4j.csv.reader.FilePlus;
import org.neo4j.function.Factory;
import org.neo4j.helpers.Args.Option;
import org.neo4j.helpers.progress.ProgressListener;
import org.neo4j.unsafe.impl.batchimport.HighestId;
//import org.neo4j.unsafe.impl.batchimport.ImportState;
import org.neo4j.unsafe.impl.batchimport.InputIterable;
import org.neo4j.unsafe.impl.batchimport.Utils.CompareType;
import org.neo4j.unsafe.impl.batchimport.cache.ByteArray;
import org.neo4j.unsafe.impl.batchimport.cache.ChunkedNumberArrayFactory;
import org.neo4j.unsafe.impl.batchimport.cache.LongArray;
import org.neo4j.unsafe.impl.batchimport.cache.MemoryStatsVisitor;
import org.neo4j.unsafe.impl.batchimport.cache.NumberArray;
import org.neo4j.unsafe.impl.batchimport.cache.NumberArrayFactory;
import org.neo4j.unsafe.impl.batchimport.cache.OffHeapIntArray;
import org.neo4j.unsafe.impl.batchimport.cache.idmapping.IdMapper;
import org.neo4j.unsafe.impl.batchimport.cache.idmapping.IdMappers;
import org.neo4j.unsafe.impl.batchimport.cache.idmapping.stringNew.ParallelSort.Comparator;
import org.neo4j.unsafe.impl.batchimport.cache.idmapping.stringNew.ParallelSort.SortWorker;
import org.neo4j.unsafe.impl.batchimport.input.Collector;
import org.neo4j.unsafe.impl.batchimport.input.Group;
import org.neo4j.unsafe.impl.batchimport.input.Groups;
import org.neo4j.unsafe.impl.batchimport.input.InputException;

import static java.lang.Math.max;
import static java.lang.Math.min;
import static org.neo4j.unsafe.impl.batchimport.Utils.unsignedCompare;
import static org.neo4j.unsafe.impl.batchimport.Utils.unsignedDifference;
import static org.neo4j.unsafe.impl.batchimport.cache.idmapping.stringNew.ParallelSort.DEFAULT;

/**
 * Maps arbitrary values to long ids. The values can be {@link #put( Object, long, Group ) added} in any order, but {@link #needsPreparation( ) needs}
 * {@link #prepare( InputIterable, Collector, ProgressListener ) preparation}
 *
 * in order to {@link #get( Object, Group ) get} ids back later.
 *
 * In the {@link #prepare( InputIterable, Collector, ProgressListener ) preparation phase} the added entries are sorted according to a number representation of
 * each input value and {@link #get( Object, Group )} does simple binary search to find the correct one.
 *
 * The implementation is space-efficient, much more so than using, say, a {@link HashMap}.
 *
 * Terminology... there's a lot going on in here, and to help you understand the code here's a list of terms used in comments and variable names and some
 * description what each generally means ( also applies to {@link ParallelSort} btw ): - input id: An id coming from the user that is associated with a neo4j id
 * by calling {@link #put( Object, long, Group )}. the first argument is the id that the user specified, the second is the neo4j id that user id will be
 * associated with. - encoder: Encodes an input id into an internal, more space efficient representation ( a {@code long} ) of that input id. - eId: The
 * internal representation of an input id, generated by an encoder. - data cache: An array of eIds. eIds are added in the order of neo4j ids, i.e. in the order
 * in which they are put. - collision: Since eId has potentially fewer bits than an input id there's a chance multiple different ( or equal ) input ids will be
 * encoded into the same eId. These are called collisions.
 */
public class EncodingIdMapper implements IdMapper
{
    public static final int COUNTING_BATCH_SIZE = 10_000;

    public interface Monitor
    {
        /**
         * @param count
         *            Number of eIds that have been marked as collisions.
         */
        void numberOfCollisions( int count );
    }

    public static final Monitor NO_MONITOR = new Monitor()
    {
        @Override
        public void numberOfCollisions( int count )
        { // Do nothing.
        }
    };
    // Bit in encoded String --> long values that marks that the particular item has
    // a collision,
    // i.e. that there's at least one other string that encodes into the same long
    // value.
    // This bit is the least significant in the most significant byte of the encoded
    // values,
    // where the 7 most significant bits in that byte denotes length of original
    // string.
    // See StringEncoder.
    private static int DEFAULT_CACHE_CHUNK_SIZE = 1_000_000; // 8MB a piece
    // Using 0 as gap value, i.e. value for a node not having an id, i.e. not
    // present in dataCache is safe
    // because the current set of Encoder implementations will always set some
    // amount of bits higher up in
    // the long value representing the length of the id.
    private static final long GAP_VALUE = 0;
    private static int ENCODING_ALTERNATES = 2; // default is 2; but could be made configurable too
    private final NumberArrayFactory cacheFactory;
    // Encoded values added in #put, in the order in which they are put. Indexes in
    // the array are the actual node ids,
    // values are the encoded versions of the input ids.
    // private final LongArray dataCache;
    private final DataCacheArray dataCache;
    private ByteArrayCache collisionCache;
    private ByteArrayCache collisionResolver;
    // Ordering information about values in dataCache; the ordering of values in
    // dataCache remains unchanged.
    // in prepare( ) this array is populated and changed along with how dataCache
    // items "move around" so that
    // they end up sorted. Again, dataCache remains unchanged, only the ordering
    // information is kept here.
    private final Encoder encoder;
    private final Radix radix;
    private final int processorsForParallelWork;
    private final Comparator comparator;
    private boolean readyForUse;
    private long[][] sortBuckets;
    private IdGroup[] idGroups = new IdGroup[10];
    private IdGroup currentIdGroup;
    private final Monitor monitor;
    private final Factory<Radix> radixFactory;
    private int chunkSize;
    private final GroupIdCache groupCache;
    private final HighestId candidateHighestSetIndex = new HighestId( -1 );
    private final Group[] groups = new Group[Groups.MAX_NUMBER_OF_GROUPS];

    private class ImportState
    {
        public File GetIDMapperFile()
        {
            return new File( "/Users/sraghuram/myws/ms/junkdb331/abcde" );
        }
    }

    ImportState importState;
    private int firstFollowerOffset;

    public EncodingIdMapper( NumberArrayFactory cacheFactory, Encoder encoder, Factory<Radix> radixFactory,
            Monitor monitor, int numberOfGroups )
    {
        this( cacheFactory, encoder, radixFactory, monitor, numberOfGroups, DEFAULT_CACHE_CHUNK_SIZE,
                Runtime.getRuntime().availableProcessors() - 1, DEFAULT );
    }

    public EncodingIdMapper( NumberArrayFactory cacheFactory, Encoder encoder, Factory<Radix> radixFactory,
            Monitor monitor, int numberOfGroups, int chunkSize, int processorsForParallelWork, Comparator comparator )
    {
        this.monitor = monitor;
        this.cacheFactory = cacheFactory;
        this.comparator = comparator;
        this.processorsForParallelWork = max( processorsForParallelWork, 1 );
        this.chunkSize = chunkSize;
        this.dataCache = new DataCacheArray();
        this.groupCache = GroupIdCache.instantiate( cacheFactory, chunkSize, numberOfGroups );
        this.encoder = encoder;
        firstFollowerOffset = 1;
        this.radixFactory = radixFactory;
        this.radix = radixFactory.newInstance();
        this.importState = new ImportState();
    }
    
    /**
     * Returns the data index ( i.e. node id ) if found, or {@code -1} if not found.
     */
    @Override
    public long get( Object inputId, Group group )
    {
        assert readyForUse;
        long returnVal = binarySearch( inputId, group.id() );
        return returnVal;
    }

    public static final int ID_SIZE = 6;

    enum ENTITY_TYPE
    {
        NORMAL, LEADER, FOLLOWER, GROUP_SIBLING, COLLISION, DUPLICATE
    }

    enum ENCODER_TYPE
    {
        STRING, LONG, UNKNOWN
    }

    ENCODER_TYPE encoderType;

    @Override
    public void put( Object inputId, long nodeId, Group group )
    {
        // Encode and add the input id
        long eId = encode( inputId );
        dataCache.setEncodedValue( nodeId, eId );
        dataCache.setNodeId( nodeId, nodeId );
        groupCache.put( nodeId, group.id() );
        candidateHighestSetIndex.offer( nodeId );

        // Store the group for later name lookup
        groups[group.id()] = group;
    }

    private long encode( Object inputId )
    {
        return encode( inputId, 0 );
    }

    private long encode( Object inputId, int version )
    {
        long eId = encoder.encode( inputId, version );
        if ( eId == GAP_VALUE )
        {
            throw new IllegalStateException( "Encoder " + encoder + " returned an illegal encoded value " + GAP_VALUE );
        }
        return eId;
    }

    private void endPreviousGroup()
    {
        if ( currentIdGroup != null )
        {
            idGroups[currentIdGroup.id()].setHighDataIndex( candidateHighestSetIndex.get() );
        }
    }

    @Override
    public boolean needsPreparation()
    {
        return true;
    }

    /**
     * There's an assumption that the progress listener supplied here can support multiple calls to started/done, and that it knows about what stages the
     * processor preparing goes through, namely:
     * <ol>
     * <li>Split by radix</li>
     * <li>Sorting</li>
     * <li>Collision detection</li>
     * <li>( potentially ) Collision resolving</li>
     * </ol>
     */
    @Override
    public void prepare( LongFunction<Object> allIds, Collector collector, ProgressListener progress )
    {
        updateRadix( dataCache, radix, candidateHighestSetIndex.get() );
        if ( allIds == null || allIds.apply( 0 ) instanceof String )
        {
            encoderType = ENCODER_TYPE.STRING;
            firstFollowerOffset = ENCODING_ALTERNATES;
        }
        else if ( allIds.apply( 0 ) instanceof Long )
        {
            encoderType = ENCODER_TYPE.LONG;
        }
        else
        {
            encoderType = ENCODER_TYPE.UNKNOWN;
        }
        endPreviousGroup();
        try
        {
            //isSorted("before sort:");
            sortBuckets = new ParallelSort( radix, dataCache, candidateHighestSetIndex.get(), processorsForParallelWork, progress,
                    comparator ).run();
            //isSorted("after sort:");
            collisionCache = new ByteArrayCache( ID_SIZE, ID_SIZE );
            int numberOfCollisions = detectAndMarkCollisions( collisionCache, progress );
            // now sort the id values in collision Cache on nodeId
            (new SortWorker( collisionCache, ID_SIZE, 0, comparator, 0, collisionCache.getSize(), progress )).run();
            if ( numberOfCollisions > 0 )
            {
                collisionResolver = new ByteArrayCache( Long.BYTES );
                buildCollisionInfo( allIds, numberOfCollisions, collector, progress, collisionCache,
                        collisionResolver );
            }
        }
        catch ( InterruptedException e )
        {
            Thread.interrupted();
            throw new RuntimeException( "Got interrupted while preparing the index. Throwing this exception "
                    + "onwards will cause a chain reaction which will cause a panic in the whole import, "
                    + "so mission accomplished" );
        }
        readyForUse = true;
        try
        {
            putOnDisk(this);
            /*EncodingIdMapper newIdMapper = (EncodingIdMapper)IdMappers.strings( cacheFactory );
            getFromDisk(newIdMapper);
            for (long i = 0; i < this.dataCache.getSize(); i++)
            {
                long eid1 = dataCache.getEncodedValue( i );
                long eid2 = newIdMapper.dataCache.getEncodedValue( i );
                long nodeid1 = dataCache.getNodeId( i );
                long nodeid2 = newIdMapper.dataCache.getNodeId( i );
                if (eid1 != eid2 || nodeid1 != nodeid2)
                    System.out.println( "Mismatch ["+ i+"]" );
            }*/
        } catch (IOException io)
        {
            
        }
    }

    private static void updateRadix( DataCacheArray values, Radix radix, long highestSetIndex )
    {
        for ( long dataIndex = 0; dataIndex <= highestSetIndex; dataIndex++ )
        {
            radix.registerRadixOf( values.getEncodedValue( dataIndex ) );
        }
    }
    
    private int radixOf( long value )
    {
        return radix.calculator().radixOf( value );
    }

    private long binarySearch( Object inputId, int groupId )
    {
        long low = 0;
        long high = candidateHighestSetIndex.get();
        long x = encode( inputId );
        if ( sortBuckets != null )
        {
            int rIndex = radixOf( x );
            for ( int k = 0; k < sortBuckets.length; k++ )
            {
                if ( rIndex <= sortBuckets[k][2] )
                {
                    low = sortBuckets[k][0];
                    high = (k == sortBuckets.length - 1) ? candidateHighestSetIndex.get() : low + sortBuckets[k][1];
                    break;
                }
            }
        }
        long returnVal = binarySearch( x, inputId, low, high, groupId );
        return returnVal;
    }

    /**
     * There are two types of collisions: - actual: collisions coming from equal input value. These might however not impose keeping original input value since
     * the colliding values might be for separate id groups, just as long as there's at most one per id space. - accidental: collisions coming from different
     * input values that happens to coerce into the same encoded value internally.
     *
     * For any encoded value there might be a mix of actual and accidental collisions. As long as there's only one such value ( accidental or actual ) per id
     * space the original input id doesn't need to be kept. For scenarios where there are multiple per for any given id space: - actual: there are two equal
     * input values in the same id space ==> fail, not allowed - accidental: there are two different input values coerced into the same encoded value in the
     * same id space ==> original input values needs to be kept
     */
    private int detectAndMarkCollisions( ByteArrayCache collisionCache, ProgressListener progress )
    {
        progress.started( "DETECT" );
        long totalCount = candidateHighestSetIndex.get() + 1;
        Workers<DetectWorker> workers = new Workers<>( "DETECT" );
        int processors = processorsForParallelWork;
        long stride = totalCount / processorsForParallelWork;
        if ( stride < 10 )
        {
            // Multi-threading would be overhead
            processors = 1;
            stride = totalCount;
        }
        long fromInclusive = 0;
        long toExclusive = 0;
        for ( int i = 0; i < processors; i++ )
        {
            boolean last = i == processors - 1;
            fromInclusive = toExclusive;
            toExclusive = last ? totalCount : toExclusive + stride;
            while ( dataCache.getEncodedValue( toExclusive ) == dataCache.getEncodedValue( toExclusive - 1 )
                    && toExclusive < totalCount )
            {
                toExclusive++;
            }
            workers.start( new DetectWorker( fromInclusive, toExclusive, last, progress, collisionCache ) );
        }
        workers.awaitAndThrowOnErrorStrict( RuntimeException.class );
        long numberOfCollisions = 0;
        for ( DetectWorker detectWorker : workers )
        {
            numberOfCollisions += detectWorker.numberOfCollisions;
        }
        progress.done();
        if ( numberOfCollisions > Integer.MAX_VALUE )
        {
            throw new InputException( "Too many collisions: " + numberOfCollisions );
        }
        int intNumberOfCollisions = java.lang.Math.toIntExact( numberOfCollisions );
        monitor.numberOfCollisions( intNumberOfCollisions );
        progress.done();
        if ( numberOfCollisions > Integer.MAX_VALUE )
        {
            throw new InputException( "Too many collisions: " + numberOfCollisions );
        }
        return intNumberOfCollisions;
    }

    private int detectAndMarkCollisions_serial( ByteArrayCache collisionCache, ProgressListener progress )
    {
        progress.started( "DETECT" );
        long numberOfCollisions = 0 + 0;
        SameGroupDetector sameGroupDetector = new SameGroupDetector();
        long sameValueStart = -1;
        boolean processCollisions = false;
        for ( long dataCacheIndex = 0; dataCacheIndex < candidateHighestSetIndex.get(); )
        {
            int batch = (int) min( candidateHighestSetIndex.get() - dataCacheIndex, COUNTING_BATCH_SIZE );
            for ( int j = 0; j < batch; j++, dataCacheIndex++ )
            {
                // isSorted( "buildcollision["+dataCacheIndex+"]" );
                long dataIndexA = dataCache.getNodeId( dataCacheIndex );
                long dataIndexB = dataCache.getNodeId( dataCacheIndex + 1 );
                if ( dataIndexA == ID_NOT_FOUND || dataIndexB == ID_NOT_FOUND )
                {
                    sameGroupDetector.reset();
                    continue;
                }
                long eIdA = dataCache.getEncodedValue( dataCacheIndex );
                long eIdB = dataCache.getEncodedValue( dataCacheIndex + 1 );
                if ( eIdA == GAP_VALUE || eIdB == GAP_VALUE )
                {
                    sameGroupDetector.reset();
                    continue;
                }
                switch ( unsignedDifference( eIdA, eIdB ) )
                {
                case EQ:
                    if ( sameValueStart == -1 )
                    {
                        sameValueStart = dataCacheIndex;
                    }
                    for ( long index = dataCacheIndex; index >= sameValueStart; index-- )
                    {
                        dataIndexA = dataCache.getNodeId( index );
                        dataIndexB = dataCache.getNodeId( index + 1 );
                        if ( dataIndexA > dataIndexB )
                        {
                            // Swap so that lower node id comes first.
                            dataCache.getByteArray().swap( index, index + 1 );
                        }
                    }
                    break;
                case GT:
                    throw new IllegalStateException(
                            "Unsorted data, a > b Failure:[" + dataCacheIndex + "] " + Long.toHexString( eIdA ) + " > "
                                    + Long.toHexString( eIdB ) + " | " + radixOf( eIdA ) + ":" + radixOf( eIdB ) );
                default:
                    if ( sameValueStart != -1 )
                    {
                        processCollisions = true;
                    }
                    sameGroupDetector.reset();
                }
                if ( sameValueStart != -1
                        && (processCollisions || dataCacheIndex == candidateHighestSetIndex.get() - 1 /* last value */ ) )
                {
                    long leaderNodeId = dataCache.getNodeId( sameValueStart );
                    long collisionCacheIndex = collisionCache.getNextIndex();
                    collisionCache.set( 1, collisionCacheIndex, leaderNodeId );
                    collisionCache.set( 2, collisionCacheIndex, sameValueStart );
                    // Leader is a normal entity type but is followed by a follower - as in this
                    // case. Leader does not have any special markings.
                    long lastIndex = dataCacheIndex == candidateHighestSetIndex.get() - 1 ? candidateHighestSetIndex.get() : dataCacheIndex;
                    for ( long followerEntry = sameValueStart + 1; followerEntry <= lastIndex; followerEntry++ )
                    {
                        long followerNodeId = dataCache.getNodeId( followerEntry );
                        dataCache.setEntityType( followerEntry, ENTITY_TYPE.FOLLOWER );
                        // for follower entries, the value is always same as its leader. Hence it is
                        // redundant and can be used to store other values.
                        // Just zero it here.
                        dataCache.setEncodedValue( followerEntry, 0 );
                        long collision = sameGroupDetector.collisionWithinSameGroup( leaderNodeId,
                                groupOf( leaderNodeId ), followerNodeId, groupOf( followerNodeId ) );
                        // group-siblings are not counted as potential collisions
                        if ( collision == ID_NOT_FOUND )
                        {
                            dataCache.setEntityType( followerEntry, ENTITY_TYPE.GROUP_SIBLING );
                        }
                        else
                        {
                            numberOfCollisions++;
                        }
                        collisionCacheIndex = collisionCache.getNextIndex();
                        collisionCache.set( 1, collisionCacheIndex, followerNodeId );
                        collisionCache.set( 2, collisionCacheIndex, followerEntry );
                    }
                    sameValueStart = -1;
                    processCollisions = false;
                }
            }
            progress.add( batch );
        }
        if ( numberOfCollisions > Integer.MAX_VALUE )
        {
            throw new InputException( "Too many collisions: " + numberOfCollisions );
        }
        monitor.numberOfCollisions( (int) numberOfCollisions );
        isSorted( "at the end of buildCollisionInfo" );
        return (int) numberOfCollisions;
    }

    public class DataCacheArray extends ByteArrayCache
    {
        public DataCacheArray()
        {
            super( 8, ID_SIZE );
        }

        public boolean isEntityType( long index, ENTITY_TYPE type )
        {
            ENTITY_TYPE myType = getEntityType( index );
            if ( myType == type )
            {
                return true;
            }
            if ( type == ENTITY_TYPE.FOLLOWER && (myType == ENTITY_TYPE.FOLLOWER || myType == ENTITY_TYPE.COLLISION
                    || myType == ENTITY_TYPE.GROUP_SIBLING || myType == ENTITY_TYPE.DUPLICATE) )
            {
                return true;
            }
            return false;
        }

        public boolean isFollower( long index )
        {
            long idValue = getValue( ID_SIZE, index, 8 );
            // if a normal entity type is followed by a follower, then it is a leader
            if ( (idValue & 0x1L) == 0 )
            {
                return false;
            }
            return true;
        }

        public boolean isNormal( long index )
        {
            long idValue = getValue( ID_SIZE, index, 8 );
            // if a normal entity type is followed by a follower, then it is a leader
            if ( (idValue & 0x1L) == 0 )
            {
                // has to be leader or normal - check the next entry
                long nextIdValue = getValue( ID_SIZE, index + 1, 8 );
                if ( (nextIdValue & 1L) != 1 )
                {
                    return true;
                }
            }
            return false;
        }

        public boolean isLeader( long index )
        {
            long idValue = getValue( ID_SIZE, index, 8 );
            // if a normal entity type is followed by a follower, then it is a leader
            if ( (idValue & 0x1L) == 0 )
            {
                // has to be leader or normal - check the next entry
                long nextIdValue = getValue( ID_SIZE, index + 1, 8 );
                if ( (nextIdValue & 1L) == 1 )
                {
                    return true;
                }
            }
            return false;
        }

        public long getNodeId( long index )
        {
            long id = getValue( ID_SIZE, index, 8 );
            return id >> 1;
        }

        public long getEncodedValue( long index )
        {
            return getValue( 8, index );
        }

        public ENTITY_TYPE getEntityType( long index )
        {
            // the LSB of the NodeId is used indicate if an entity is normal entity or
            // follower ( that has
            // same encoded value of a normal entity ). Only for followers the LSB is set.
            long idValue = getValue( ID_SIZE, index, 8 );
            // if a normal entity type is followed by a follower, then it is a leader
            if ( (idValue & 0x1L) == 0 )
            {
                // has to be leader or normal - check the next entry
                long nextIdValue = getValue( ID_SIZE, index + 1, 8 );
                if ( (nextIdValue & 1L) == 1 )
                {
                    return ENTITY_TYPE.LEADER;
                }
                return ENTITY_TYPE.NORMAL;
            }
            // it is a follower. get its type - stored in 8th byte of encoded value.
            byte followerType = byteArray.getByte( index, 7 );
            if ( followerType == ENTITY_TYPE.GROUP_SIBLING.ordinal() )
            {
                return ENTITY_TYPE.GROUP_SIBLING;
            }
            if ( followerType == ENTITY_TYPE.COLLISION.ordinal() )
            {
                return ENTITY_TYPE.COLLISION;
            }
            if ( followerType == ENTITY_TYPE.DUPLICATE.ordinal() )
            {
                return ENTITY_TYPE.DUPLICATE;
            }
            return ENTITY_TYPE.FOLLOWER;
        }

        public int getEncodingType( long index )
        {
            return (int) byteArray.getByte( index, 6 );
        }

        public long getCollisionInfo( long index )
        {
            return getValue( ID_SIZE, index );
        }

        // -----
        public void setNodeId( long index, long value )
        {
            set( 2, index, (value << 1) + (get( 2, index ) % 2) );
        }

        public void setEncodedValue( long index, long value )
        {
            set( 1, index, value );
        }

        public void setEntityType( long index, ENTITY_TYPE entryType )
        {
            long value = get( 2, index );
            switch ( entryType )
            {
            case LEADER:
            case NORMAL:
                /// if it was a follower, then clear it. Otherwise, nothing is needed.
                if ( value % 2 == 1 )
                {
                    set( 2, index, value - 1 );
                }
                break;
            case FOLLOWER:
            case GROUP_SIBLING:
            case COLLISION:
            case DUPLICATE:
                // first, set it as follower and then set the follower type.
                if ( value % 2 == 0 )
                {
                    set( 2, index, value + 1 );
                }
                if ( entryType != ENTITY_TYPE.FOLLOWER )
                {
                    byte followerType = byteArray.getByte( index, 7 );
                    followerType = (byte) (followerType | entryType.ordinal());
                    byteArray.setByte( index, 7, followerType );
                }
                break;
            default:
                break;
            }
        }

        public void setEncodingType( long index, int value )
        {
            byteArray.setByte( index, 6, (byte) value );
        }

        public void setCollisionInfo( long index, long value )
        {
            setValue( ID_SIZE, index, value );
        }
    }

    public class ByteArrayCache
    {
        protected ByteArray byteArray;
        private int numEntries;
        private int[] entrySizes;
        private int sizeInBytesPerEntry;
        int[] offset;
        private long currentIndex;

        public ByteArrayCache( int... entrySize )
        {
            for ( int i = 0; i < entrySize.length; i++ )
            {
                sizeInBytesPerEntry += entrySize[i];
            }
            offset = new int[entrySize.length];
            offset[0] = 0;
            for ( int i = 1; i < entrySize.length; i++ )
            {
                offset[i] = offset[i - 1] + entrySize[i - 1];
            }
            entrySizes = entrySize;
            numEntries = entrySize.length;
            byteArray = cacheFactory.newDynamicByteArray( chunkSize, new byte[sizeInBytesPerEntry] );
        }

        public boolean isEntityType( long index, ENTITY_TYPE type )
        {
            return true;
        }

        public ByteArray getByteArray()
        {
            return byteArray;
        }

        public long set( long index, long value )
        {
            return set( 1, index, value );
        }

        public long set( int entryNumber, long index, long value )
        {
            setValue( entrySizes[entryNumber - 1], index, offset[entryNumber - 1], value );
            return index;
        }

        public long get( long index )
        {
            return get( 1, index );
        }

        public long get( int entryNumber, long index )
        {
            return getValue( entrySizes[entryNumber - 1], index, offset[entryNumber - 1] );
        }

        public int getNumEntries()
        {
            return numEntries;
        }

        public long getSize()
        {
            return currentIndex;
        }
        
        public long getSizeInBytes()
        {
            return currentIndex * sizeInBytesPerEntry;
        }
        public void incrementSize()
        {
            currentIndex++;
        }
        public long getNextIndex()
        {
            return getNextIndex( 1 );
        }

        public synchronized long getNextIndex( int count )
        {
            long returnVal = currentIndex;
            currentIndex += count;
            return returnVal;
        }

        public long getValue( long index )
        {
            return getValue( 8, index, 0 );
        }

        public long getValue( int length, long index )
        {
            return getValue( length, index, 0 );
        }

        public long getValue( int length, long index, int offset )
        {
            switch ( length )
            {
            case 1:
                return byteArray.getByte( index, offset );
            case 2:
                return byteArray.getShort( index, offset );
            case 3:
                return byteArray.get3ByteInt( index, offset );
            case 4:
                return byteArray.getInt( index, offset );
            case 5:
                return byteArray.get5ByteLong( index, offset );
            case 6:
                return byteArray.get6ByteLong( index, offset );
            case 8:
            default:
                return byteArray.getLong( index, offset );
            }
        }

        public void setValue( long index, long value )
        {
            setValue( 8, index, 0, value );
        }

        public void setValue( int length, long index, long value )
        {
            setValue( length, index, 0, value );
        }

        public void setValue( int length, long index, int offset, long value )
        {
            switch ( length )
            {
            case 2:
                byteArray.setShort( index, offset, (short) value );
                break;
            case 3:
                byteArray.set3ByteInt( index, offset, (int) value );
                break;
            case 4:
                byteArray.setInt( index, offset, (int) value );
            case 5:
                byteArray.set5ByteLong( index, offset, (int) value );
                break;
            case 6:
                byteArray.set6ByteLong( index, offset, value );
                break;
            case 8:
            default:
                byteArray.setLong( index, offset, value );
            }
            if (index > currentIndex)
                currentIndex = index;
        }

        public void close()
        {
            byteArray.close();
        }
    }

    private void buildCollisionInfo( LongFunction<Object> allIds, int numberOfCollisions, Collector collector,
            ProgressListener progress, ByteArrayCache collisionCache, ByteArrayCache collisionInfo )
            throws InterruptedException
    {
        progress.started( "RESOLVE ( " + numberOfCollisions + " collisions )" );
        long collisionInfoIndex = 0;
        for ( long collisionCacheIndex = 0; collisionCacheIndex < collisionCache.getSize(); )
        {
            long j = 0;
            for ( ; j < COUNTING_BATCH_SIZE
                    && collisionCacheIndex < collisionCache.getSize(); j++, collisionCacheIndex++ )
            {
                long nodeId = collisionCache.get( collisionCacheIndex );
                long dataCacheIndex = collisionCache.get( 2, collisionCacheIndex );
                assert nodeId == dataCache.getNodeId( dataCacheIndex );
                int collisionInfoEntrySize = 0;
                if ( dataCache.isLeader( dataCacheIndex ) )
                {
                    if ( encoderType == ENCODER_TYPE.STRING )
                    {
                        String idString = (String)allIds.apply( nodeId );
                        for ( int k = 0; k < ENCODING_ALTERNATES; k++ )
                        {
                            long eid = encode( idString, k + 1 );
                            collisionInfo.set( collisionInfoIndex + collisionInfoEntrySize++, eid );
                        }
                    }
                    else if ( encoderType == ENCODER_TYPE.LONG )
                    {
                        collisionInfo.set( collisionInfoIndex + collisionInfoEntrySize++,
                                (long) allIds.apply( nodeId ));
                    }
                    else
                    {
                        //TO DO - throw unknown type exception
                    }
                    // save the leader's collisionInfoIndex in the first follower dataCache entry
                    dataCache.setCollisionInfo( dataCacheIndex + 1, collisionInfoIndex );
                    // for the first follower allow an entry in the collisionInfo.
                    collisionInfoEntrySize++;
                }
                else
                {
                    // follower - there are 3 possibilities:
                    // 1. it could be a duplicate with a different group id - group sibling
                    // 2. it is a true duplicate with all encoded values same
                    // 3. it is a collision - because all encodings are not same. There is a
                    // Theoretically, it is possible that collision could have different
                    // encodings same, but the probability is too low. We use configurable number
                    // different encodings to make is nearly impossible. The best
                    // way is to save the actual string Ids and comparing them but will need more memory.
                    // first, get the leader index of this block in dataCache
                    long leaderDataCacheIndex = getToLeader( dataCache, dataCacheIndex );
                    long leaderNodeId = dataCache.getNodeId( leaderDataCacheIndex );
                    // get the entry point into the collisionInfo for the leader which will be
                    // used to determine duplicates as well as in first follower.
                    long leaderCollisionInfoIndex = dataCache.getCollisionInfo( leaderDataCacheIndex + 1 );
                    
                    // first check for collisions - check the alternate encodings
                    boolean isCollision = false;
                    int encodingType = -1;
                    long alternateEncodingValue = 0;
                    String idString = null;
                    if ( encoderType == ENCODER_TYPE.STRING )
                    {
                        // now get the id string
                        idString = (String)allIds.apply( nodeId );
                        for ( int k = 0; k < ENCODING_ALTERNATES; k++ )
                        {
                            alternateEncodingValue = encode( idString, k + 1 );
                            if ( !unsignedCompare( alternateEncodingValue,
                                    collisionInfo.get( leaderCollisionInfoIndex + k ), CompareType.EQ ) )
                            {
                                encodingType = k + 1;
                                isCollision = true;
                                break;
                            }
                        }
                    }
                    else if ( encoderType == ENCODER_TYPE.LONG )
                    {
                        if ( collisionInfo.get( leaderCollisionInfoIndex ) != (long) allIds.apply( nodeId ) )
                        {
                            isCollision = true;
                        }
                    }
                    else
                    {
                        // TO DO - throw exception
                    }
                    if ( !isCollision )
                    {
                        // group-sibling or duplicate
                        // first check if the groupIds are different before concluding it as duplicate
                        int leaderGroupId = groupOf( leaderNodeId );
                        int groupId = groupOf( nodeId );
                        if ( groupId != leaderGroupId )
                        {
                            dataCache.setEntityType( dataCacheIndex, ENTITY_TYPE.GROUP_SIBLING );
                        }
                        else
                        {
                            dataCache.setEntityType( dataCacheIndex, ENTITY_TYPE.DUPLICATE );
                            collector.collectDuplicateNode( idString, nodeId, groups[groupId].name());
                        }
                    }
                    else
                    { // collision case
                        dataCache.setEntityType( dataCacheIndex, ENTITY_TYPE.COLLISION );
                        if ( encoderType == ENCODER_TYPE.STRING )
                        {
                            // the resolving alternateEncodingValue value is already computed.
                            // save the encoding type
                            dataCache.setEncodingType( dataCacheIndex, encodingType );
                        }
                        else if ( encoderType == ENCODER_TYPE.LONG )
                        {
                            alternateEncodingValue =
                                    (((long) groupOf( nodeId )) << 48) | (long) allIds.apply( nodeId );
                        }
                        else
                        {
                            //TO DO - throw exception
                        }
                    }
                    // save the collision resolving value in collisionInfo and its pointer
                    // in the dataCache
                    if ( dataCacheIndex - leaderDataCacheIndex == 1 )
                    {
                        // first follower - the collisionInfoIndex is ALWAYS fixed distance
                        //     after the entries of leader in collisionInfo
                        collisionInfo.set( leaderCollisionInfoIndex + firstFollowerOffset, alternateEncodingValue );
                    }
                    else
                    {
                        // non-first followers
                        collisionInfo.set( collisionInfoIndex + collisionInfoEntrySize++, alternateEncodingValue );
                        dataCache.setCollisionInfo( dataCacheIndex, collisionInfoIndex );
                    }
                }
                collisionInfoIndex += collisionInfoEntrySize;
                collisionInfoEntrySize = 0;
                progress.add( j );
            }
        }
        progress.done();
    }

    private int groupOf( long dataIndex )
    {
        return groupCache.get( dataIndex );
    }

    private long getToLeader( DataCacheArray cache, long index )
    {
        while ( cache.isFollower( index ) && index >= 0 )
        {
            index--;
        }
        return index;
    }

    private long binarySearch( long x, Object inputId, long low, long high, int groupId )
    {
        while ( low <= high )
        {
            long mid = low + (high - low) / 2;
            long dataCacheIndex = getToLeader( dataCache, mid );
            if ( dataCacheIndex == ID_NOT_FOUND )
            {
                return ID_NOT_FOUND;
            }
            long midValue = dataCache.getEncodedValue( dataCacheIndex );
            switch ( unsignedDifference( midValue, x ) )
            {
            case EQ:
                boolean equal = mid != dataCacheIndex
                        || (dataCacheIndex < candidateHighestSetIndex.get() && dataCache.isFollower( dataCacheIndex + 1 ));
                if ( equal )
                { // OK so there are actually multiple equal data values here, we need to go through them all
                  // to be sure we find the correct one.
                    return findFromEIdRange( dataCacheIndex, midValue, inputId, x, groupId );
                }
                // This is the only value here, let's do a simple comparison with correct group id and return
                long nodeId = dataCache.getNodeId( dataCacheIndex );
                return groupOf( nodeId ) == groupId ? nodeId : ID_NOT_FOUND;
            case LT:
                low = mid + 1;
                break;
            default:
                high = mid - 1;
                break;
            }
        }
        return ID_NOT_FOUND;
    }

    private long findFromEIdRange( long index, long val, Object inputId, long x, int groupId )
    {
        // there is a equal value block to be checked.
        long toIndex = index + 1;
        while ( toIndex < candidateHighestSetIndex.get() && dataCache.isFollower( toIndex ) )
        {
            toIndex++;
        }
        long leaderCollisionInfoCacheIndex = dataCache.getCollisionInfo( index + 1 );
        long leaderNodeId = dataCache.getNodeId( index );
        ENTITY_TYPE entryType = ENTITY_TYPE.LEADER;// assume as leader
        if ( collisionResolver != null )
        {
            try
            {
                if ( encoderType == ENCODER_TYPE.STRING )
                {
                    // check the alternate encodings
                    for ( int j = 0; j < ENCODING_ALTERNATES; j++ )
                    {
                        if ( encode( inputId, j + 1 ) != collisionResolver.get( leaderCollisionInfoCacheIndex + j ) )
                        {
                            entryType = ENTITY_TYPE.COLLISION;
                            break;
                        }
                    }
                }
                else if ( encoderType == ENCODER_TYPE.LONG )
                {
                    if ( collisionResolver.get( leaderCollisionInfoCacheIndex ) != (long) inputId )
                    {
                        entryType = ENTITY_TYPE.COLLISION;
                    }
                }
            }
            catch ( ArrayIndexOutOfBoundsException ae )
            {
                System.out.println( "ArrayIndexOutOfBoundsException" );
            }
        }
        if ( entryType == ENTITY_TYPE.LEADER )
        {
            if ( groupOf( leaderNodeId ) == groupId )
            {
                return leaderNodeId;
            }
            entryType = ENTITY_TYPE.GROUP_SIBLING;
        }
        // for group sibling or collisions - we have to search all the values with same
        // eid
        for ( long i = index + 1; i <= toIndex; i++ )
        {
            long nodeId = dataCache.getNodeId( i );
            switch ( dataCache.getEntityType( i ) )
            {
            case GROUP_SIBLING:
                if ( entryType == ENTITY_TYPE.GROUP_SIBLING && groupOf( nodeId ) == groupId )
                {
                    return nodeId;
                }
                break;
            case COLLISION:
                if ( entryType == ENTITY_TYPE.COLLISION && collisionResolver != null )
                {
                    long newVal = encoderType == ENCODER_TYPE.STRING
                            ? encode( inputId, dataCache.getEncodingType( i ) )
                            : (((long) groupId << 48) | ((long) inputId));
                    long collisionInfoIndex = (i == index + 1)
                            ? leaderCollisionInfoCacheIndex + firstFollowerOffset
                            : dataCache.getCollisionInfo( i );
                    //System.out.println("looking for[" + val + "," + collisionInfoIndex + " = " +  groupId  + " + " + newVal + "]");
                    if ( unsignedCompare( collisionResolver.get( collisionInfoIndex ), newVal, CompareType.EQ ) )
                    {
                        return nodeId;
                    }
                }
                break;
            case NORMAL:
            case LEADER:
            case FOLLOWER:
            case DUPLICATE:// ignore
                break;
            default:
                break;
            }
        }
        return ID_NOT_FOUND;
    }

    @Override
    public void acceptMemoryStatsVisitor( MemoryStatsVisitor visitor )
    {
        nullSafeAcceptMemoryStatsVisitor( visitor, dataCache.getByteArray() );
    }

    private void nullSafeAcceptMemoryStatsVisitor( MemoryStatsVisitor visitor, MemoryStatsVisitor.Visitable mem )
    {
        if ( mem != null )
        {
            mem.acceptMemoryStatsVisitor( visitor );
        }
    }

    @Override
    public String toString()
    {
        return getClass().getSimpleName() + "[" + encoder + "," + radix + "]";
    }

    @Override
    public void close()
    {
        dataCache.getByteArray().close();
    }

    private class DetectWorker implements Runnable
    {
        private final long fromInclusive;
        private final long toExclusive;
        private final boolean last;
        private final ProgressListener progress;
        private int numberOfCollisions;
        private int localProgress;
        private ByteArrayCache collisionCache;

        DetectWorker( long fromInclusive, long toExclusive, boolean last, ProgressListener progress,
                ByteArrayCache collisionCache )
        {
            this.fromInclusive = fromInclusive;
            this.toExclusive = toExclusive;
            this.last = last;
            this.progress = progress;
            this.collisionCache = collisionCache;
        }

        @Override
        public void run()
        {
            SameGroupDetector sameGroupDetector = new SameGroupDetector();
            // In all chunks except the last this chunk also takes care of the detection in
            // the seam,
            // but for the last one there's no seam at the end.
            long end = last ? toExclusive - 1 : toExclusive;
            long sameValueStart = -1;
            boolean processCollisions = false;
            for ( long dataCacheIndex = fromInclusive; dataCacheIndex < end; )
            {
                int batch = (int) min( candidateHighestSetIndex.get() - dataCacheIndex, COUNTING_BATCH_SIZE );
                for ( int j = 0; j < batch && dataCacheIndex < end; j++, dataCacheIndex++ )
                {
                    long dataIndexA = dataCache.getNodeId( dataCacheIndex );
                    long dataIndexB = dataCache.getNodeId( dataCacheIndex + 1 );
                    if ( dataIndexA == ID_NOT_FOUND || dataIndexB == ID_NOT_FOUND )
                    {
                        sameGroupDetector.reset();
                        continue;
                    }
                    long eIdA = dataCache.getEncodedValue( dataCacheIndex );
                    long eIdB = dataCache.getEncodedValue( dataCacheIndex + 1 );
                    if ( eIdA == GAP_VALUE || eIdB == GAP_VALUE )
                    {
                        sameGroupDetector.reset();
                        continue;
                    }
                    switch ( unsignedDifference( eIdA, eIdB ) )
                    {
                    case EQ:
                        if ( sameValueStart == -1 )
                        {
                            sameValueStart = dataCacheIndex;
                        }
                        for ( long index = dataCacheIndex; index >= sameValueStart; index-- )
                        {
                            dataIndexA = dataCache.getNodeId( index );
                            dataIndexB = dataCache.getNodeId( index + 1 );
                            if ( dataIndexA > dataIndexB )
                            {
                                // Swap so that lower node id comes first.
                                dataCache.getByteArray().swap( index, index + 1 );
                            }
                        }
                        break;
                    case GT:
                        throw new IllegalStateException( "Parallel - Unsorted data, a > b Failure:[" + dataCacheIndex
                                + "] " + Long.toHexString( dataCache.getEncodedValue( dataCacheIndex ) ) + " > "
                                + Long.toHexString( dataCache.getEncodedValue( dataCacheIndex + 1 ) ) + " | "
                                + radixOf( eIdA ) + ":" + radixOf( eIdB ) );
                    default:
                        if ( sameValueStart != -1 )
                        {
                            processCollisions = true;
                        }
                        sameGroupDetector.reset();
                    }
                    if ( sameValueStart != -1 && (processCollisions || dataCacheIndex == end - 1 /* last value */ ) )
                    {
                        long collisionCacheIndex = collisionCache.getNextIndex();
                        long leaderNodeId = dataCache.getNodeId( sameValueStart );
                        collisionCache.set( 1, collisionCacheIndex, leaderNodeId );
                        collisionCache.set( 2, collisionCacheIndex, sameValueStart );
                        dataCache.setEntityType( sameValueStart, ENTITY_TYPE.LEADER );
                        long lastIndex = dataCacheIndex == candidateHighestSetIndex.get() - 1 ? candidateHighestSetIndex.get() : dataCacheIndex;
                        for ( long followerEntry = sameValueStart + 1; followerEntry <= lastIndex; followerEntry++ )
                        {
                            long followerNodeId = dataCache.getNodeId( followerEntry );
                            dataCache.setEntityType( followerEntry, ENTITY_TYPE.FOLLOWER );
                            // for follower entries, the value is always same as its leader. Hence it is
                            // redundant and can be used to store other values.
                            // Just zero it here, for now.
                            dataCache.setEncodedValue( followerEntry, 0 );
                            long collision = sameGroupDetector.collisionWithinSameGroup( leaderNodeId,
                                    groupOf( leaderNodeId ), followerNodeId, groupOf( followerNodeId ) );
                            // group-siblings are not counted as potential collisions
                            if ( collision == ID_NOT_FOUND )
                            {
                                dataCache.setEntityType( followerEntry, ENTITY_TYPE.GROUP_SIBLING );
                            }
                            else
                            {
                                numberOfCollisions++;
                            }
                            collisionCacheIndex = collisionCache.getNextIndex();
                            collisionCache.set( 1, collisionCacheIndex, followerNodeId );
                            collisionCache.set( 2, collisionCacheIndex, followerEntry );
                        }
                        sameValueStart = -1;
                        processCollisions = false;
                    }
                    if ( ++localProgress == 1000 )
                    {
                        progress.add( localProgress );
                        localProgress = 0;
                    }
                }
                progress.add( localProgress );
            }
        }
    }
    // -------------------------

    public static void putOnDisk( IdMapper idMapper ) throws IOException
    {
        if ( !(idMapper instanceof EncodingIdMapper) )
        {
            return;
        }
        EncodingIdMapper eIdMapper = (EncodingIdMapper) idMapper;
        long position = 0;
        position = putOnDiskCache( eIdMapper.importState, eIdMapper.dataCache, position, false );
        System.out.println( "\nState saved:" +  eIdMapper.dataCache.getSizeInBytes() + "-" + position );
    }

    public static void getFromDisk( EncodingIdMapper idMapper ) throws IOException
    {
        if ( !(idMapper instanceof EncodingIdMapper) )
        {
            return;
        }
        EncodingIdMapper eIdMapper = (EncodingIdMapper) idMapper;
        long position = 0;
        position = getFromDiskCache( eIdMapper.importState, eIdMapper.dataCache, position );
        System.out.println( "State Read2:" +  eIdMapper.dataCache.getSize() );
    }

    private static long putOnDiskCache( ImportState importState, ByteArrayCache cache, long newPosition,
            boolean append ) throws IOException
    {
        FileOutputStream fos = new FileOutputStream( importState.GetIDMapperFile().getAbsolutePath(), append );
        FileChannel channel = fos.getChannel();
        channel = channel.position( newPosition );
        long position = channel.position();
        ByteBuffer bb = ByteBuffer.allocateDirect( 8 );
        long numEntries = cache.getSizeInBytes();
        bb.putLong( numEntries );
        bb.flip();
        int bytesWritten = channel.write( bb );
        bb.clear();
        bb.putLong( ID_SIZE );
        bb.flip();
        bytesWritten = channel.write( bb );
        position = channel.position();
        if ( numEntries > 0 )
        {
            bb = ByteBuffer.allocateDirect( 8 * 8 * 1024 );
            byte tmp = 0;
            for ( long i = 0; i < numEntries; i += 8 * 1024 )
            {
                boolean over = false;
                for ( int j = 0; j < 8192 && (i + j) < numEntries && !over; j++ )
                {
                    tmp = cache.getByteArray().getByteRaw(  i+ j);
                    if ( tmp != 0 || (i + j) < numEntries )
                    {
                        bb.put( tmp );
                    }
                    else
                    {
                        over = true;
                    }
                }
                bb.flip();
                channel.write( bb );
                position = channel.position();
                bb.clear();
            }
        }
        position = channel.position();
        channel.close();
        fos.close();
        return position;
    }

    static final int size64k = 1024 * 64, size8k = 8 * 1024;

    private static long getFromDiskCache( ImportState importState, ByteArrayCache cache, long newPosition ) throws IOException
    {
        FileInputStream fis = new FileInputStream( importState.GetIDMapperFile() );
        FileChannel channel = fis.getChannel();
        long position = channel.position();
        channel = channel.position( newPosition );
        ByteBuffer bb = ByteBuffer.allocateDirect( 8 );
        int bytesRead = channel.read( bb );
        bb.flip();
        long cacheSize = bb.getLong();
        bb.clear();
        channel.read( bb );
        bb.flip();
        int itemSize = (int)bb.getLong() + 8;
        if ( cacheSize > 0 )
        {
            ByteBuffer buffer8k = ByteBuffer.allocateDirect( size8k );
            ByteBuffer lastBuffer = ByteBuffer.allocateDirect( (int) (cacheSize) % size8k );
            for ( long i = 0; i < cacheSize; i += size8k )
            {
                bb = ((cacheSize - i) >= size8k) ? buffer8k : lastBuffer;
                channel.read( bb );
                position = channel.position();
                bb.flip();
                try
                {
                    for ( int j = 0; j < size8k && (i + j) < cacheSize; j++ )
                    {
                        cache.getByteArray().putByteRaw( i+j , bb.get() );
                        if (i+j > 0 && (i+j) % itemSize == 0)
                            cache.incrementSize();
                    }
                }
                catch ( Exception e )
                {
                    System.out.println( e.getMessage() );
                }
                bb.clear();
            }
        }
        position = channel.position();
        fis.close();
        return position;
    }

    public static void main( String[] args ) throws IOException
    {
        // ImportState importState = new ImportState( new File( "/Users/SRaghuram/temp" ),
        // null, null );
        // long length = Long.parseLong( args[0] );
        Encoder encoder = new StringEncoder();
        // ChunkedNumberArrayFactory arrayFactory = new ChunkedNumberArrayFactory( );
        // testDataCache( importState, length, arrayFactory );
    }

/*
    
    private static void testDataCache( ImportState importState, long length, ChunkedNumberArrayFactory arrayFactory )
            throws IOException
    {
        EncodingIdMapper idMapper =
                new EncodingIdMapper( arrayFactory, new LongEncoder(), Radix.LONG, NO_MONITOR, dynamic() );
        EncodingIdMapper idMapperNew =
                new EncodingIdMapper( arrayFactory, new LongEncoder(), Radix.LONG, NO_MONITOR, dynamic() );
        EncodingIdMapper idMapperNew1 =
                new EncodingIdMapper( arrayFactory, new LongEncoder(), Radix.LONG, NO_MONITOR, dynamic() );
        Random rand = new Random( 123456789 );
        for ( int i = 0; i < length; i++ )
            if ( i % 3 == 0 )
                idMapper.dataCache.set( i, 0 );
            else
                idMapper.dataCache.set( i, rand.nextLong() );
        System.out.println( "Cache Length:" + length );
        long position = 0;
        long start = System.currentTimeMillis();
        System.out.println( "Start:" );
        long position1 = putOnDiskCache( importState, idMapper.dataCache, length, position, false );
        System.out.println( "LongsWritten:[" + position1 + "] in [" + (System.currentTimeMillis() - start) + "]ms" );
        position = putOnDiskCache( importState, idMapper.dataCache, length, position1, true );
        System.out.println( "LongsWritten:[" + position + "] in [" + (System.currentTimeMillis() - start) + "]ms" );
        start = System.currentTimeMillis();
        position = getFromDiskCache( importState, idMapperNew.dataCache, 0 );
        System.out.println( "LongsRead:[" + position + "] in [" + (System.currentTimeMillis() - start) + "]ms" );
        checkLongArray( "DataCache1", idMapper.dataCache, idMapperNew.dataCache );
        position = getFromDiskCache( importState, idMapperNew1.dataCache, position1 );
        System.out.println( "LongsRead:[" + position + "] in [" + (System.currentTimeMillis() - start) + "]ms" );
        checkLongArray( "DataCache2", idMapper.dataCache, idMapperNew1.dataCache );
    }
*/
    static void checkLongArray( String prefix, LongArray d1, LongArray d2 )
    {
        if ( d1 == null || d2 == null )
        {
            return;
        }
        for ( int i = 0; i < d1.length(); i++ )
        {
            if ( d1.get( i ) != d2.get( i ) )
            {
                System.out.println( prefix + ":[" + i + "][" + d1.get( i ) + "][" + d2.get( i ) + "]" );
            }
        }
    }

    static void checkOffHeapByteArray( String prefix, NumberArray d1, NumberArray d2 )
    {
        OffHeapIntArray d3 = null, d4 = null;
        if ( d1 == null || d2 == null )
        {
            return;
        }
        if ( d1 instanceof OffHeapIntArray )
        {
            d3 = (OffHeapIntArray) d1;
        }
        if ( d2 instanceof OffHeapIntArray )
        {
            d4 = (OffHeapIntArray) d2;
        }
        for ( int i = 0; i < d1.length(); i++ )
        {
            if ( d3.get( i ) != d4.get( i ) )
            {
                System.out.println( prefix + ":[" + i + "][" + d3.get( i ) + "][" + d4.get( i ) + "]" );
            }
        }
    }

    public void duplicate() throws IOException
    {
        EncodingIdMapper.putOnDisk( this );
        EncodingIdMapper newIdMapper =
                new EncodingIdMapper( this.cacheFactory, new LongEncoder(), Radix.LONG, NO_MONITOR, chunkSize );
        EncodingIdMapper.getFromDisk( newIdMapper );
        // checkLongArray( "DataCache",this.dataCache, newIdMapper.dataCache );
    }

    public void find( String msg, String value )
    {
        long index = find( value );
        System.out.println( msg + ":[ " + index + " ]-" + dataCache.isLeader( index ) );// dataCache.isLeader(
        // index ) );
    }

    public long find( String value )
    {
        //
        long eid = encode( value );
        for ( long i = 0; i < candidateHighestSetIndex.get(); i++ )
        {
            if ( dataCache.getEncodedValue( i ) == eid )
            {
                return i;
            }
        }
        return -1;
    }

    public void isSorted( String msg )
    {
        isSorted( msg, dataCache, 8, 0, true, candidateHighestSetIndex.get() );
    }

    public void isSorted( String msg, boolean leaderExists )
    {
        isSorted( msg, dataCache, 8, 0, leaderExists, candidateHighestSetIndex.get() );
    }

    public void isSorted( String msg, DataCacheArray cache, int entrySize, int offset, boolean leaderExists, long max )
    {
        long index = 0 + 0;
        for ( long i = 0; i < max; )
        {
            long val1 = cache.getEncodedValue( i );
            long nextVal = i + 1;
            while ( leaderExists && cache.isFollower( nextVal ) && nextVal < max )
            {
                nextVal++;
            }
            long val2 = cache.getEncodedValue( nextVal );
            if ( val1 > val2 && nextVal < max )
            {
                System.out.println( msg + "[" + max + "]-----Not Sorted[" + i + "][" + val1 + "]--[" + nextVal + "]["
                        + val2 + "]" );
                return;
            }
            i = nextVal;
            index = nextVal;
        }
        System.out.println( msg + "[" + max + "] -----Sorted-Sorted [" + index + "]" );
    }

    @Override
    public long calculateMemoryUsage( long numberOfNodes )
    {
        return numberOfNodes * (Long.BYTES + ID_SIZE);
    }

    @Override
    public PrimitiveLongIterator leftOverDuplicateNodesIds()
    {
        if ( collisionCache == null || collisionCache.getSize() <= 0 )
        {
            return PrimitiveLongCollections.emptyIterator();
        }
        // Scans duplicate marks in tracker cache. There is no bit left in dataCache to store this bit so we use
        // the tracker cache as if each index into it was the node id.
        return new PrimitiveLongCollections.PrimitiveLongBaseIterator()
        {
            private long nodeId;
            private long collisionCacheIndex;

            @Override
            protected boolean fetchNext()
            {
                while ( collisionCacheIndex <= collisionCache.getSize() )
                {
                    long candidate = collisionCacheIndex++;
                    //if ( trackerCache.isMarkedAsDuplicate( candidate ) )
                    long dataCacheIndex = collisionCache.get( 2, candidate );
                    if ( dataCache.isEntityType( dataCacheIndex, ENTITY_TYPE.DUPLICATE ) )
                    {
                        return next( dataCache.getNodeId( dataCacheIndex ) );
                    }
                }
                collisionCache.close();
                return false;
            }
        };
    }
}
