/*
 * Copyright (c) 2002-2017 "Neo Technology,"
 * Network Engine for Objects in Lund AB [http://neotechnology.com]
 *
 * This file is part of Neo4j.
 *
 * Neo4j is free software: you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation, either version 3 of the License, or
 * (at your option) any later version.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License
 * along with this program.  If not, see <http://www.gnu.org/licenses/>.
 */
package org.neo4j.unsafe.impl.batchimport.cache.idmapping.string;

import java.io.FileInputStream;
import java.io.FileOutputStream;
import java.io.IOException;
import java.nio.ByteBuffer;
import java.nio.channels.FileChannel;
import java.util.Arrays;
import java.util.Collection;
import java.util.HashMap;
import java.util.Iterator;

import org.neo4j.function.Factory;
import org.neo4j.helpers.progress.ProgressListener;
import org.neo4j.kernel.api.labelscan.LabelScanWriter;
import org.neo4j.kernel.impl.store.NodeLabelsField;
import org.neo4j.kernel.impl.store.NodeStore;
import org.neo4j.kernel.impl.store.PropertyStore;
import org.neo4j.kernel.impl.store.record.NodeRecord;
import org.neo4j.kernel.impl.store.record.PropertyBlock;
import org.neo4j.kernel.impl.store.record.PropertyRecord;
import org.neo4j.kernel.impl.store.record.RecordLoad;
import org.neo4j.unsafe.impl.batchimport.ImportState;
import org.neo4j.unsafe.impl.batchimport.InputIterable;
import org.neo4j.unsafe.impl.batchimport.InputIterator;
import org.neo4j.unsafe.impl.batchimport.Utils.CompareType;
import org.neo4j.unsafe.impl.batchimport.cache.ByteArray;
import org.neo4j.unsafe.impl.batchimport.cache.LongArray;
import org.neo4j.unsafe.impl.batchimport.cache.MemoryStatsVisitor;
import org.neo4j.unsafe.impl.batchimport.cache.NumberArray;
import org.neo4j.unsafe.impl.batchimport.cache.NumberArrayFactory;
import org.neo4j.unsafe.impl.batchimport.cache.OffHeapIntArray;
import org.neo4j.unsafe.impl.batchimport.cache.idmapping.IdMapper;
import org.neo4j.unsafe.impl.batchimport.cache.idmapping.string.IdGroup;
import org.neo4j.unsafe.impl.batchimport.cache.idmapping.string.ParallelSort.Comparator;
import org.neo4j.unsafe.impl.batchimport.cache.idmapping.string.ParallelSort.SortWorker;
import org.neo4j.unsafe.impl.batchimport.input.Collector;
import org.neo4j.unsafe.impl.batchimport.input.Group;
import org.neo4j.unsafe.impl.batchimport.input.InputException;
import org.neo4j.unsafe.impl.batchimport.store.BatchingNeoStores;

import static java.lang.Math.max;
import static java.lang.Math.min;
import static org.neo4j.collection.primitive.PrimitiveLongCollections.EMPTY_LONG_ARRAY;
import static org.neo4j.kernel.api.labelscan.NodeLabelUpdate.labelChanges;
import static org.neo4j.unsafe.impl.batchimport.Utils.unsignedCompare;
import static org.neo4j.unsafe.impl.batchimport.Utils.unsignedDifference;
import static org.neo4j.unsafe.impl.batchimport.cache.idmapping.string.ParallelSort.DEFAULT;


/**
 * Maps arbitrary values to long ids. The values can be {@link #put(Object, long, Group) added} in any order,
 * but {@link #needsPreparation() needs} {@link #prepare(InputIterable, Collector, ProgressListener) preparation}
 *
 * in order to {@link #get(Object, Group) get} ids back later.
 *
 * In the {@link #prepare(InputIterable, Collector, ProgressListener) preparation phase} the added entries are sorted according to a number representation
 * of each input value and {@link #get(Object, Group)} does simple binary search to find the correct one.
 *
 * The implementation is space-efficient, much more so than using, say, a {@link HashMap}.
 *
 * Terminology... there's a lot going on in here, and to help you understand the code here's a list
 * of terms used in comments and variable names and some description what each generally means
 * (also applies to {@link ParallelSort} btw):
 * - input id:
 *       An id coming from the user that is associated with a neo4j id by calling {@link #put(Object, long, Group)}.
 *       the first argument is the id that the user specified, the second is the neo4j id that user id will
 *       be associated with.
 * - encoder:
 *       Encodes an input id into an internal, more space efficient representation (a {@code long}) of that input id.
 * - eId:
 *       The internal representation of an input id, generated by an encoder.
 * - data cache:
 *       An array of eIds. eIds are added in the order of neo4j ids, i.e. in the order in which they are put.
 * - collision:
 *       Since eId has potentially fewer bits than an input id there's a chance multiple different (or equal)
 *       input ids will be encoded into the same eId. These are called collisions.
 */
public class EncodingIdMapper implements IdMapper
{
    static final int COUNTING_BATCH_SIZE = 10_000;

    public interface Monitor
    {
        /**
         * @param count Number of eIds that have been marked as collisions.
         */
        void numberOfCollisions( int count );
    }

    public static final Monitor NO_MONITOR = new Monitor()
    {
        @Override
        public void numberOfCollisions( int count )
        {   // Do nothing.
        }
    };

    // Bit in encoded String --> long values that marks that the particular item has a collision,
    // i.e. that there's at least one other string that encodes into the same long value.
    // This bit is the least significant in the most significant byte of the encoded values,
    // where the 7 most significant bits in that byte denotes length of original string.
    // See StringEncoder.
  
    private static int DEFAULT_CACHE_CHUNK_SIZE = 10_000_000; // 8MB a piece
    // Using 0 as gap value, i.e. value for a node not having an id, i.e. not present in dataCache is safe
    // because the current set of Encoder implementations will always set some amount of bits higher up in
    // the long value representing the length of the id.
    private static final long GAP_VALUE = 0;
    private static int ENCODING_ALTERNATES = 2; // default is 2; but could be made configurable too

    private final NumberArrayFactory cacheFactory;
    // Encoded values added in #put, in the order in which they are put. Indexes in the array are the actual node ids,
    // values are the encoded versions of the input ids.
    //private final LongArray dataCache;
    private final ByteArray dataCache;
    private long highestSetIndex = -1;

    // Ordering information about values in dataCache; the ordering of values in dataCache remains unchanged.
    // in prepare() this array is populated and changed along with how dataCache items "move around" so that
    // they end up sorted. Again, dataCache remains unchanged, only the ordering information is kept here.
    private final Encoder encoder;
    private final Radix radix;
    private final int processorsForSorting;
    private final Comparator comparator;

    private boolean readyForUse;
    private long[][] sortBuckets;

    private IdGroup[] idGroups = new IdGroup[10];
    private IdGroup currentIdGroup;
    private final Monitor monitor;
    private final Factory<Radix> radixFactory;
    private int chunkSize = 0;
    private BatchingNeoStores neoStore;
    ImportState importState;
    private int[] keyIdList = new int[100];
    private int currentKeyIndex = -1;
    private long[] keyRange = new long[100];
    final static int size64k = 1024*64, size8k = 8*1024;
    

    public EncodingIdMapper( NumberArrayFactory cacheFactory, Encoder encoder, Factory<Radix> radixFactory,
            Monitor monitor, BatchingNeoStores neoStore )
    {
        this( cacheFactory, encoder, radixFactory, monitor, DEFAULT_CACHE_CHUNK_SIZE,
                Runtime.getRuntime().availableProcessors() - 1, DEFAULT, neoStore );
    }

    public EncodingIdMapper( NumberArrayFactory cacheFactory, Encoder encoder, Factory<Radix> radixFactory,
            Monitor monitor, int chunkSize, int processorsForSorting,
            Comparator comparator, BatchingNeoStores neoStore )
    {
        this.monitor = monitor;
        this.cacheFactory = cacheFactory;
        this.comparator = comparator;
        this.processorsForSorting = max( processorsForSorting, 1 );
        //this.dataCache = cacheFactory.newDynamicLongArray( chunkSize, GAP_VALUE );
        this.dataCache = cacheFactory.newDynamicByteArray(chunkSize, new byte[ENTRYSIZE]);
        this.encoder = encoder;
        this.radixFactory = radixFactory;
        this.radix = radixFactory.newInstance();
        this.chunkSize = chunkSize;
        this.neoStore = neoStore;
    }

    /**
     * Returns the data index (i.e. node id) if found, or {@code -1} if not found.
     */
    @Override
    public long get( Object inputId, Group group )
    {
        assert readyForUse;
        long returnVal = binarySearch(  inputId, group.id() );
        return returnVal;
    }

 
    public static final int ID_SIZE = 6;
    public static final int ENTRYSIZE = ID_SIZE + 8;

    public static long getValue(ByteArray cache, long index)
    {
    		return getValue(cache, 8, index, 0);
    }
    public static long getValue(ByteArray cache, int length, long index)
    {
    		return getValue(cache, length, index, 0);
    }
    public static long getValue(ByteArray cache, int length, long index, int offset)
    {
    		switch (length)
    		{
    		case 1: 
    			return cache.getByte(index, offset);
    		case 2:
    			return cache.getShort(index, offset);
    		case 3:
    			return cache.get3ByteInt(index, offset);
    		case 4:
    			return cache.getInt(index, offset);
    		case 6:
    			return cache.get6ByteLong(index, offset);
    		case 8:
    		default:
    			return cache.getLong(index, offset);
    		}
    }
    public static long getId(ByteArray cache, long index)
    {
    		long id = getValue(cache, ID_SIZE, index, 8);
    		if (id < 0)
    			return -id;
    		return id;
    }
    
    public boolean isLeader(ByteArray cache, long index)
    {
    		if (getEntryType(cache, index) == ENTRY_TYPE.LEADER)
    			return true;
    		return false;
    }
    
    public int getEncodingType(ByteArray cache, long index)
    {
    		return (int)cache.getByte(index, 6);
    }
    
    enum ENTRY_TYPE
    {
    		LEADER, FOLLOWER, GROUP_SIBLING, COLLISION, DUPLICATE  			
    }
    
    public ENTRY_TYPE getEntryType(ByteArray cache, long index)
    {
    		long value = getValue(cache, ID_SIZE, index, 8);
    		if ( value >= 0)
    			return ENTRY_TYPE.LEADER;
    		byte followerType = cache.getByte(index, 7);
    		if (followerType == ENTRY_TYPE.GROUP_SIBLING.ordinal())
    			return ENTRY_TYPE.GROUP_SIBLING;
    		if (followerType == ENTRY_TYPE.COLLISION.ordinal())
    			return ENTRY_TYPE.COLLISION;
    		if (followerType == ENTRY_TYPE.DUPLICATE.ordinal())
    			return ENTRY_TYPE.DUPLICATE;
    		return ENTRY_TYPE.FOLLOWER;
    }
    public void setEntryType(ByteArray cache, long index, ENTRY_TYPE entryType)
    {
    		long value = getValue(cache, ID_SIZE, index, 8);		
    		switch (entryType)
    		{
    		case LEADER:	
    			if (value < 0)
    				setValue(cache, ID_SIZE, index, 8, -value);
        		break;
    		case FOLLOWER:
    		case GROUP_SIBLING:
    		case COLLISION:
    		case DUPLICATE:
    			if (value >= 0)
    				setValue(cache, ID_SIZE, index, 8, -value);
    			if (entryType != ENTRY_TYPE.FOLLOWER)
    			{
	    			byte followerType = cache.getByte(index, 7);
	    			followerType = (byte)(followerType | entryType.ordinal());
	    			cache.setByte(index, 7, followerType);
    			}
    			break;
    		}
    }
    
    public void setEncodingType(ByteArray cache, long index, int value)
    {
    		cache.setByte(index,  7, (byte)(value));
    }
   
    public static void setValue(ByteArray cache, long index, long value)
    {
    		setValue(cache, 8, index, 0, value);
    }
    public static void setValue(ByteArray cache, int length, long index, long value)
    {
    		setValue(cache, length, index, 0, value);
    }
    public static void setValue(ByteArray cache, int length, long index, int offset, long value)
    {

    		switch (length)
    		{
    		case 2:
    			 cache.setShort(index, offset, (short)value);  
    			 break;
    		case 3:
    			 cache.set3ByteInt(index, offset, (int)value);
    			 break;
    		case 4:
    			 cache.setInt(index, offset, (int)value);
    		//case 5:
   		//	 cache.set5ByteInt(index, offset, (int)value);
    		//	 break;
    		case 6:
    			 cache.set6ByteLong(index, offset, value);
    			 break;
    		case 8:
    		default:
    			 cache.setLong(index, offset, value);
    		}
    }
    public static void setId(ByteArray cache, long id, long value)
    {
    		cache.set6ByteLong(id, 8, value);
    }
    public static void dataCacheSwap(ByteArray cache, long i, long j)
    {
    		cache.swap(i, j);
    }

    private void saveIdKey(long id, int keyId)
    {
    		if (currentKeyIndex != -1 && keyIdList[currentKeyIndex] == keyId)
    			return;
    		currentKeyIndex++;
    		keyIdList[currentKeyIndex] = keyId;
    		if (currentKeyIndex != 0)
    			keyRange[currentKeyIndex-1] = id;		
    }
    private int findKeyId(long id)
    {
    		for (int i = 0; i < currentKeyIndex; i++)
    			if (id < keyRange[i])
    				return keyIdList[i];
    		return keyIdList[currentKeyIndex];
    }
    
    @Override
    public void put( Object inputId, long id, Group group )
    {
    		put(inputId, null, id, group);
    }

    @Override
    public void put( Object inputId, String idName, long id, Group group )
    {
        // Fill any gap to the previously highest set id
        for ( long gapId = highestSetIndex + 1; gapId < id; gapId++ )
        {
            radix.registerRadixOf( GAP_VALUE );
        }

        // Check if we're now venturing into a new group. If so then end the previous group.
        int groupId = group.id();
        boolean newGroup = false;
        if ( currentIdGroup == null )
        {
            newGroup = true;
        }
        else
        {
            if ( groupId < currentIdGroup.id() )
            {
                throw new IllegalStateException( "Nodes for any specific group must be added in sequence " +
                        "before adding nodes for any other group" );
            }
            newGroup = groupId != currentIdGroup.id();
        }
        if ( newGroup )
        {
            endPreviousGroup();
        }

        // Encode and add the input id
        long eId = encode( inputId );
        /// based on the radix, figure the bucket to put the eid
        setValue( dataCache, id, eId );
        setId( dataCache, id, id );
        
        if (idName != null)
        		saveIdKey(id, neoStore.getPropertyKeyRepository().getOrCreateId(idName));

        highestSetIndex = id;
        radix.registerRadixOf( eId );

        // Create the new group
        if ( newGroup )
        {
            if ( groupId >= idGroups.length )
            {
                idGroups = Arrays.copyOf( idGroups, max( groupId + 1, idGroups.length * 2 ) );
            }
            idGroups[groupId] = currentIdGroup = new IdGroup( group, id );
        }
    }

    private long encode( Object inputId )
    {
    		return encode(inputId, 0);
    }
    private long encode( Object inputId, int version )
    {
        long eId = encoder.encode( inputId, version );
        if ( eId == GAP_VALUE )
        {
            throw new IllegalStateException( "Encoder " + encoder + " returned an illegal encoded value " + GAP_VALUE );
        }
        return eId;
    }

    private void endPreviousGroup()
    {
        if ( currentIdGroup != null )
        {
            idGroups[currentIdGroup.id()].setHighDataIndex( highestSetIndex );
        }
    }

    @Override
    public boolean needsPreparation()
    {
        return true;
    }
    
    /**
     * There's an assumption that the progress listener supplied here can support multiple calls
     * to started/done, and that it knows about what stages the processor preparing goes through, namely:
     * <ol>
     * <li>Split by radix</li>
     * <li>Sorting</li>
     * <li>Collision detection</li>
     * <li>(potentially) Collision resolving</li>
     * </ol>
     */
    @Override
    public void prepare( InputIterable<Object> ids, Collector collector, ProgressListener progress )
    {
        endPreviousGroup();
        try
        {
            sortBuckets = new ParallelSort( radix, dataCache, highestSetIndex,
                    processorsForSorting, progress, comparator ).run();
            ByteArray[] collisionCaches = new ByteArray[1];
            long[] collisionCacheIndex = new long[1];
            int numberOfCollisions = detectAndMarkCollisions( collisionCaches, collisionCacheIndex, progress );
            if ( numberOfCollisions > 0 )
            {
                try ( InputIterator<Object> idIterator = ids.iterator() )
                {
                    buildCollisionInfo( idIterator, numberOfCollisions, collector, progress, collisionCaches, collisionCacheIndex );
                }
            }
        }
        catch ( InterruptedException e )
        {
            Thread.interrupted();
            throw new RuntimeException( "Got interrupted while preparing the index. Throwing this exception "
                    + "onwards will cause a chain reaction which will cause a panic in the whole import, "
                    + "so mission accomplished" );
        }
        readyForUse = true;
    }


    private int radixOf( long value )
    {
        return radix.calculator().radixOf( value );
    }
    
    private long binarySearch( Object inputId, int groupId )
    {
        long low = 0;
        long high = highestSetIndex;
        long x = encode( inputId );
        int rIndex = radixOf( x );
        for ( int k = 0; k < sortBuckets.length; k++ )
        {
            if ( rIndex <= sortBuckets[k][0] )//bucketRange[k] > rIndex )
            {
                low = sortBuckets[k][2];
                high = (k == sortBuckets.length - 1) ? highestSetIndex : sortBuckets[k + 1][2];
                break;
            }
        }

        long returnVal = binarySearch( x, inputId, low, high, groupId );
        if ( returnVal == ID_NOT_FOUND )
        {
            low = 0;
            high = highestSetIndex;
            returnVal = binarySearch( x, inputId, low, high, groupId );
        }
        
        return returnVal;
    }

    /**
     * There are two types of collisions:
     * - actual: collisions coming from equal input value. These might however not impose
     *   keeping original input value since the colliding values might be for separate id groups,
     *   just as long as there's at most one per id space.
     * - accidental: collisions coming from different input values that happens to coerce into
     *   the same encoded value internally.
     *
     * For any encoded value there might be a mix of actual and accidental collisions. As long as there's
     * only one such value (accidental or actual) per id space the original input id doesn't need to be kept.
     * For scenarios where there are multiple per for any given id space:
     * - actual: there are two equal input values in the same id space
     *     ==> fail, not allowed
     * - accidental: there are two different input values coerced into the same encoded value
     *   in the same id space
     *     ==> original input values needs to be kept
     */
    private int detectAndMarkCollisions( ByteArray[] collisionCaches, long[] collisionCacheIndexes, ProgressListener progress )
    {
        progress.started( "DETECT" );
        long numberOfCollisions = 0;
        SameGroupDetector sameGroupDetector = new SameGroupDetector();
        long sameValueStart = -1;
        ByteArray collisionCache = cacheFactory.newDynamicByteArray(chunkSize, new byte[ID_SIZE*2]);
        
        long collisionCacheIndex = 0;
        boolean processCollisions = false;
        for ( long dataCacheIndex = 0; dataCacheIndex < highestSetIndex; )
        {
            int batch = (int) min( highestSetIndex - dataCacheIndex, COUNTING_BATCH_SIZE );
            for ( int j = 0; j < batch; j++, dataCacheIndex++ )
            {
                long dataIndexA = getId( dataCache, dataCacheIndex);
                long dataIndexB = getId( dataCache, dataCacheIndex+1);
                if ( dataIndexA == ID_NOT_FOUND || dataIndexB == ID_NOT_FOUND )
                {
                    sameGroupDetector.reset();
                    continue;
                }

                long eIdA = getValue( dataCache, dataCacheIndex );
                long eIdB = getValue( dataCache, dataCacheIndex+1);
                if ( eIdA == GAP_VALUE || eIdB == GAP_VALUE )
                {
                    sameGroupDetector.reset();
                    //continue;
                }

                switch ( unsignedDifference( eIdA, eIdB ) )
                {              
	                case EQ:
	                		if (sameValueStart == -1)
	                			sameValueStart = dataCacheIndex;
	
	                    for (long index = dataCacheIndex; index >= sameValueStart; index--)
	                    {
	                    		dataIndexA = getId( dataCache, index);
	                    		dataIndexB = getId( dataCache, index+1);
		                    if ( dataIndexA > dataIndexB )
		                        // Swap so that lower node id comes first.                   		
		                    		 EncodingIdMapper.dataCacheSwap( dataCache,  index, index + 1 );
	                    }
	                    
	                    numberOfCollisions++;
	                    break;
	                case GT: //throw new IllegalStateException
	        				System.out.println( "Unsorted data, a > b Failure:[" + dataCacheIndex + "] " +
	        						Long.toHexString( eIdA ) + " > " + Long.toHexString( eIdB ) + " | " +
	        						radixOf( eIdA ) + ":" + radixOf( eIdB ) );
	                default:
	                		if (sameValueStart != -1 )
	                			processCollisions = true;
	                    sameGroupDetector.reset();
                }
                if (processCollisions || dataCacheIndex == highestSetIndex-1 /* last value*/)
        			{
                		long nodeId = getId( dataCache, sameValueStart);
	        			setValue(collisionCache, ID_SIZE, collisionCacheIndex, nodeId);
	        			setValue(collisionCache, ID_SIZE, collisionCacheIndex, ID_SIZE, sameValueStart);
	        			collisionCacheIndex++;
	        			setEntryType(dataCache, sameValueStart, ENTRY_TYPE.LEADER);
	        			
	        			for (long followerEntry = sameValueStart+1; followerEntry <= dataCacheIndex; followerEntry++)
	        			{
	        				setEntryType(dataCache, followerEntry, ENTRY_TYPE.FOLLOWER);
	        				//for follower entries, the value is always same as its leader. Hence it is redundant and can be used to store other values.
	        				//Just zero it here.        				
	        				nodeId = getId( dataCache, followerEntry);
	        				setValue(dataCache, followerEntry, 0);
	        				setValue(collisionCache, ID_SIZE, collisionCacheIndex, nodeId);
	        				setValue(collisionCache, ID_SIZE, collisionCacheIndex, ID_SIZE, followerEntry);
	        				collisionCacheIndex++;
	        			}

	        			sameValueStart = -1;
	        			processCollisions = false;
        			}		
            }
            progress.add( batch );
            
        }

        // now sort the id values in both firstCollisions and followerCollisions caches.
        (new SortWorker( collisionCache, ID_SIZE, 0, comparator, 0, collisionCacheIndex+1, progress )).run();
        progress.done();

        if ( numberOfCollisions > Integer.MAX_VALUE )
        {
            throw new InputException( "Too many collisions: " + numberOfCollisions );
        }

        monitor.numberOfCollisions( (int) numberOfCollisions );
        collisionCaches[0] = collisionCache;
        collisionCacheIndexes[0] = collisionCacheIndex;
        return (int) numberOfCollisions;
    }
   
    private void buildCollisionInfo( InputIterator<Object> ids, int numberOfCollisions,
            Collector collector, ProgressListener progress, ByteArray[] collisionCaches, long[] collisionCacheIndex )
            throws InterruptedException
    {
        progress.started( "RESOLVE (" + numberOfCollisions + " collisions)" );
        ByteArray collisionCache = collisionCaches[0];
        long collisionCacheMaxIndex = collisionCacheIndex[0]+0;
        ByteArray collisionResolvingInfo = cacheFactory.newDynamicByteArray(chunkSize, new byte[8*ENCODING_ALTERNATES]);
        long collisionResolvingInfoIndex = 0;
        for (long i = 0; i <= collisionCacheMaxIndex; i++)
        {
	        	long dataCacheIndex = getValue(collisionCache, ID_SIZE, i, ID_SIZE );
	        	long nodeId = getId(dataCache, dataCacheIndex);
	        	long nodeId1 = getValue(collisionCache, ID_SIZE, i);
	        	
	        	assert nodeId == nodeId1;
	        	   	
	        	if (isLeader(dataCache, dataCacheIndex))
	        	{
		        	String idString = getIdString(nodeId);
		        	// encode the string into ENCODING_ALTERNATES number of ways
		        	for (int k = 0; k < ENCODING_ALTERNATES; k++)
		        	{
		        		long eid = encode(idString, k+1);
		        		setValue(collisionResolvingInfo, 8, collisionResolvingInfoIndex, k*8, eid);
		        	}
		        	//save the leader index and collisionResolvingInfoIndex
		        	setValue(dataCache, ID_SIZE, dataCacheIndex+1, i);
		        	setValue(collisionCache, ID_SIZE, i, collisionResolvingInfoIndex);
		        	collisionResolvingInfoIndex++;
		        	continue;
	        	}
	        	else
	        {
	        		// follower
	        		// there are 3 possibilities:
	    	        // 1. it could be a duplicate but group id is different
	    	        // 2. it is a true duplicate with all encoded values same 
	    	        // 3. it is a collision - because all encodings are not same. There is a theoritical possibility that collision could have different
	        		//	   encodings same, but the probability is too low. We use 2 or more different encodings to make is nearly impossible. The best
	        		//	   way is to save the actual string Ids and comparing them. 

	        		// first, get the leader index of this block in dataCache
	        		long leaderDataCacheIndex = getToLeader(dataCache, dataCacheIndex);
	        		
		        	// get the leader index in collision Cache that is stored in the value entry of next index (because all value entries of these items are equal)
		    		long leaderCollisionIndex = getValue(dataCache, ID_SIZE, leaderDataCacheIndex+1);
		    		long leaderNodeId = getId(dataCache, leaderDataCacheIndex);
		    		long collisionInfoIndex = getValue(collisionCache, ID_SIZE, leaderCollisionIndex);
		    		NodeRecord node = new NodeRecord( nodeId ); 
		    		// now get the id string
		    		String idString = getIdString(nodeId);
		    		boolean isDuplicate = true;
		    		int encodingType = -1;
		    		for (int k = 0; k < ENCODING_ALTERNATES; k++)
		    			if (!unsignedCompare(encode(idString, k+1), getValue(collisionResolvingInfo, 8, collisionInfoIndex, k*8), CompareType.EQ))
		    			{
		    				encodingType = k;
		    				isDuplicate = false;
		    				break;
		    			}
		    		if (isDuplicate)
		    		{
		    			// first check if the groupIds are different before concluding it as duplicate
		    			int leaderGroupId = groupOf( leaderNodeId ).id();
		    			int groupId = groupOf( nodeId ).id();
		    			if (groupId != leaderGroupId)
		    				setEntryType(dataCache, dataCacheIndex, ENTRY_TYPE.GROUP_SIBLING);
		    			else
		    			{
		    				setEntryType(dataCache, dataCacheIndex, ENTRY_TYPE.DUPLICATE);
		    				// these are true duplicate - group it at the end and delete from the node store
		    				try 
		    				{
		    					deleteDuplicateNode( node );
		    				} catch (IOException io)
		    				{
		    					// may be retry???? Yet to be done. 
		    				}
		    			}
		    		}
		    		else
		    		{
		    			setEntryType(dataCache, dataCacheIndex, ENTRY_TYPE.COLLISION);
		    			setEncodingType(dataCache, dataCacheIndex, encodingType);
		    		}
	        }
        }
        
        progress.done();
    }

    
    private void deleteDuplicateNode(NodeRecord node) throws IOException
    {
    		NodeStore nodeStore = neoStore.getNodeStore();
    		LabelScanWriter labelScanWriter = neoStore.getLabelScanStore().newWriter();
    		long[] labels = NodeLabelsField.get( node, nodeStore );
		node.setInUse( false );
		nodeStore.updateRecord( node );
		if ( labels.length > 0 )
		{
			try {
				labelScanWriter.write( labelChanges( node.getId(), labels, EMPTY_LONG_ARRAY ) );
			} catch (IOException ie)
			{
				System.out.println("Error in writing to label scan store - "+ ie.getMessage());
			}
		}	
		labelScanWriter.close();
    }
    
    private String getIdString(long nodeId)
    {
    		NodeRecord node = new NodeRecord( nodeId+0 );        		
    		neoStore.getNodeStore().getRecord(nodeId, node, RecordLoad.NORMAL);
		if (!node.inUse())
			return null;
		else
		{
    			Collection<PropertyRecord> propCollect = neoStore.getPropertyStore().getPropertyRecordChain(node.getNextProp());
    			Iterator<PropertyRecord> propI = propCollect.iterator();
    			while (propI.hasNext())
    			{
    				PropertyRecord prop = propI.next();	 
    				PropertyBlock pb = prop.getPropertyBlock(findKeyId(node.getId()));
    				if (pb != null)
    				{
    					String idString = (String)pb.getType().value(pb, neoStore.getPropertyStore()).asObject();
    					if (idString == null)
    						idString = pb.newPropertyValue(neoStore.getPropertyStore()).toString();
    					return idString;
    				}
    			}
		}
		return null;
    }


    private IdGroup groupOf( long dataIndex )
    {
        for ( IdGroup idGroup : idGroups )
        {
            if ( idGroup != null && idGroup.covers( dataIndex ) )
            {
                return idGroup;
            }
        }
        throw new IllegalArgumentException( "Strange, index " + dataIndex + " isn't included in a group" );
    }

    private long getToLeader(ByteArray cache, long index)
    {
    		while ( !isLeader(cache, index) && index >=0)
    			index--;
    		return index;
    }
    
    private long binarySearch( long x, Object inputId, long low, long high, int groupId )
    {
        while ( low <= high )
        {
            long mid = low + (high - low) / 2;
            long dataCacheIndex = getToLeader(dataCache, mid);
            mid = dataCacheIndex;
            if ( dataCacheIndex == ID_NOT_FOUND )
            {
                return ID_NOT_FOUND;
            }
            long midValue = getValue( dataCache, dataCacheIndex );
            switch ( unsignedDifference( midValue, x ) )
            {
            case EQ:
                // We found the value we were looking for. Question now is whether or not it's the only
                // of its kind. The next set of values that are followers will form the group of same values.
                return findFromEIdRange( mid, midValue, inputId, x, groupId );
            case LT:
                low = nextEntry(mid, true);
                break;
            default:
                high = nextEntry(mid, false);
                break;
            }
        }
        return ID_NOT_FOUND;
    }
    
    private long nextEntry(long index, boolean direction)
    {
    		int increment = direction ? 1 : -1;
    		index += increment;
    		while (index >= 0 && index < highestSetIndex && !isLeader(dataCache, index))
    			index += increment;
    		if (direction)
    			return index >= 0 ? index: 0;
    		else
    			return index < highestSetIndex ? index : highestSetIndex;
    		
    }

    private long findFromEIdRange( long index, long val, Object inputId, long x, int groupId )
    {
        assert val == x;

        long toIndex = index + 1;
        while ( toIndex < highestSetIndex && !isLeader(dataCache, toIndex))
        		toIndex++;    

        for (long i = index; i < toIndex; i++)
        {
        		long nodeId = getId(dataCache, i);
        		switch (getEntryType(dataCache, index))
        		{
        		case LEADER:
        			if (groupOf( nodeId ).id() == groupId)
    					return nodeId;
        			break;
        		case GROUP_SIBLING:
        			if (groupOf( nodeId ).id() != groupId)
    					return nodeId;
        			break;
        		case COLLISION:
        			long newVal = encode(inputId, getEncodingType(dataCache, i));
        			if (unsignedCompare( val, newVal, CompareType.EQ))
        				return nodeId;
        			break;
        		case FOLLOWER:
        		case DUPLICATE://do nothing
        			break;
        		}
        }
        return ID_NOT_FOUND;
    }

    @Override
    public void acceptMemoryStatsVisitor( MemoryStatsVisitor visitor )
    {
        nullSafeAcceptMemoryStatsVisitor( visitor, dataCache );
    }

    private void nullSafeAcceptMemoryStatsVisitor( MemoryStatsVisitor visitor, MemoryStatsVisitor.Visitable mem )
    {
        if ( mem != null )
        {
            mem.acceptMemoryStatsVisitor( visitor );
        }
    }

    @Override
    public String toString()
    {
        return getClass().getSimpleName() + "[" + encoder + "," + radix + "]";
    }

    @Override
    public void close()
    {
        dataCache.close();
    }


	public static void putOnDisk(IdMapper idMapper) throws IOException{
		if (!(idMapper instanceof EncodingIdMapper))
			return;
		EncodingIdMapper eIdMapper = (EncodingIdMapper)idMapper;
		long position = 0;
		position = putOnDiskCache(eIdMapper.importState,  eIdMapper.dataCache, position, false );
		System.out.println("State saved:"+ printLength("DataCahce", eIdMapper.dataCache)); 
	}


	public static void getFromDisk(EncodingIdMapper idMapper) throws IOException{
		if (!(idMapper instanceof EncodingIdMapper))
			return;
		EncodingIdMapper eIdMapper = (EncodingIdMapper)idMapper;
		long position = 0;
		System.out.println("State Read1:"+ printLength("DataCahce", eIdMapper.dataCache) );
		position = getFromDiskCache(eIdMapper.importState,  eIdMapper.dataCache, position );
		System.out.println("State Read2:"+ printLength("DataCahce", eIdMapper.dataCache) );
	}
	private static String printLength( String msg, Object cache)
	{
		long length = -1l;
		if (cache == null)
			length = 0;
		if (cache instanceof LongArray)
			length = ((LongArray)cache).length();
		return "["+msg +":"+length+"]";
	}
	
	private static long putOnDiskCache(ImportState importState, Object cache, long newPosition, boolean append) throws IOException
	{
		long length = -1l;
		if (cache == null)
			length = 0;
		if (cache instanceof LongArray)
			length = ((LongArray)cache).length();
		return putOnDiskCache(importState, cache, length, newPosition, append );
	}
	private static long putOnDiskCache(ImportState importState, Object cache, long numEntries, long newPosition, boolean append) throws IOException
	{
		FileOutputStream fos = new FileOutputStream( importState.GetIDMapperFile().getAbsolutePath(), append );
		FileChannel channel = fos.getChannel();
		channel = channel.position(newPosition);
		long position = channel.position();
		ByteBuffer bb = ByteBuffer.allocateDirect(8);
		bb.putLong(numEntries);
		bb.flip();
		int bytesWritten = channel.write(bb);
		position = channel.position();
		if (numEntries > 0)
		{
			bb = ByteBuffer.allocateDirect(8*8*1024);
			long tmp = 0;
			for (long i = 0; i < numEntries; i += 8*1024)
			{	
				boolean over = false;
				for (int j = 0; j < 8192 && (i + j) < numEntries && !over ; j++)
				{
					if (cache instanceof LongArray)
						tmp = ((LongArray)cache).get(i + j);				
					
					if (tmp != 0 || (i+j) < numEntries)
						bb.putLong(tmp);
					else
						over = true;
				}
				bb.flip();
				channel.write(bb);
				position = channel.position();
				bb.clear();
			}
		}
		position = channel.position();
		channel.close();
		fos.close();
		return position;
	}

	
	private static long getFromDiskCache(ImportState importState, Object cache, long newPosition) throws IOException
	{
		FileInputStream fis = new FileInputStream(importState.GetIDMapperFile());	  
		FileChannel channel = fis.getChannel();
		long position = channel.position();
		channel = channel.position(newPosition);
		ByteBuffer bb = ByteBuffer.allocateDirect(8);
		int bytesRead = channel.read(bb);
		bb.flip();
		long cacheSize = bb.getLong();
		if (cacheSize > 0)
		{
			ByteBuffer buffer64k = ByteBuffer.allocateDirect(size64k);
			ByteBuffer lastBuffer = ByteBuffer.allocateDirect((int)(cacheSize * 8) % size64k);
			for (long i = 0; i < cacheSize; i += size8k)
			{
				bb = ((cacheSize - i) >= size8k) ? buffer64k : lastBuffer;
				channel.read(bb);
				position = channel.position();
				bb.flip();
				try
				{
					for (int j = 0; j < size8k && (i + j) < cacheSize; j++)
					{
						if (cache instanceof LongArray)
							((LongArray)cache).set(i + j, bb.getLong());					
					}
				} catch (Exception e)
				{
					System.out.println(e.getMessage());
				}
				bb.clear();
			}
		}
		position = channel.position();
		fis.close();
		return position;
	}

	static void checkLongArray(String prefix, LongArray d1, LongArray d2)
	{
		if (d1 == null || d2 == null)
			return;
		for (int i = 0; i < d1.length(); i++)
			if (d1.get(i) != d2.get(i))
				System.out.println(prefix +":["+i+"]["+d1.get(i)+"]["+d2.get(i)+"]");
	}

	public void duplicate() throws IOException
    {
		EncodingIdMapper.putOnDisk(this);
    		EncodingIdMapper newIdMapper = new EncodingIdMapper( this.cacheFactory, new LongEncoder(), Radix.LONG, NO_MONITOR, neoStore );
    		EncodingIdMapper.getFromDisk(newIdMapper);
    }
}
